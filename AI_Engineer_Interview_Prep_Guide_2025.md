# AI Engineer Interview Preparation Guide 2025
## Comprehensive Study Materials for AI Engineering Positions

---

## Executive Summary

This comprehensive interview preparation guide is designed for candidates targeting AI Engineer, GenAI Engineer, and Agentic AI Engineer positions at companies ranging from startups to Big Tech (FAANG+).

**Key Focus Areas for 2025:**
- **Agentic AI & Multi-Agent Systems** - The dominant trend and most critical emerging skill
- **Production LLM Experience** - Foundational requirement, not experimental
- **MLOps/LLMOps/AgenticOps** - Production-first mindset expected
- **RAG & Vector Databases** - Standard infrastructure knowledge
- **Function Calling & Tool Use** - Critical for agent autonomy

**Preparation Timeline:**
- **Minimum (Foundational)**: 4-6 weeks intensive study
- **Recommended (Competitive)**: 8-12 weeks comprehensive preparation
- **Optimal (Senior/Specialist)**: 12-16 weeks with hands-on projects

**Study Approach:**
1. **Weeks 1-3**: Core fundamentals (Python, ML/DL, frameworks)
2. **Weeks 4-6**: LLMs, GenAI, and production deployment
3. **Weeks 7-9**: AI Agents, multi-agent systems, agentic frameworks
4. **Weeks 10-12**: System design, MLOps, and hands-on projects
5. **Weeks 13-16** (if time): Advanced topics, portfolio building, mock interviews

---

## Table of Contents

1. [Programming & Computer Science Fundamentals](#1-programming--computer-science-fundamentals)
2. [AI/ML Frameworks & Deep Learning](#2-aiml-frameworks--deep-learning)
3. [Large Language Models (LLMs) & Generative AI](#3-large-language-models-llms--generative-ai)
4. [AI Agents & Agentic AI](#4-ai-agents--agentic-ai)
5. [RAG, Vector Databases & Embeddings](#5-rag-vector-databases--embeddings)
6. [Cloud Platforms & MLOps](#6-cloud-platforms--mlops)
7. [System Design for AI Systems](#7-system-design-for-ai-systems)
8. [Behavioral Questions](#8-behavioral-questions)
9. [Hands-on Coding Challenges](#9-hands-on-coding-challenges)
10. [Study Resources & Timeline](#10-study-resources--timeline)

---

# 1. Programming & Computer Science Fundamentals

## 1.1 Python Programming

### Concept Definition
Python is the dominant programming language for AI/ML development (71% of job postings), used extensively for building neural networks, training models, data processing, and deploying AI applications. Mastery includes understanding Python's object-oriented features, functional programming paradigms, concurrency models, and ecosystem of AI/ML libraries.

### Rationale & Industry Relevance
- **Universal Language**: Python is the de facto standard across TensorFlow, PyTorch, LangChain, AutoGen, and virtually all AI frameworks
- **Production Systems**: Companies like OpenAI, Anthropic, Google use Python for both research and production AI systems
- **Rapid Prototyping**: Python's expressiveness allows quick experimentation, critical in fast-moving AI research
- **Career Impact**: Python proficiency is non-negotiable; 71% of AI Engineer positions explicitly require it

### Pros & Cons

**Advantages:**
- Rich ecosystem of AI/ML libraries (NumPy, Pandas, scikit-learn, PyTorch, TensorFlow)
- Easy to read and write, facilitating collaboration
- Strong community support and extensive documentation
- Excellent for rapid prototyping and experimentation

**Limitations:**
- Performance constraints for compute-intensive tasks (though mitigated by C++ backends)
- Global Interpreter Lock (GIL) limits true multi-threading
- Not ideal for mobile or edge deployment (often requires conversion to C++/TensorFlow Lite)
- Dynamic typing can lead to runtime errors in production

**Common Pitfalls:**
- Treating Python as a beginner language; advanced features (decorators, context managers, metaclasses) are expected
- Not understanding memory management and reference counting
- Inefficient use of loops instead of vectorized operations (NumPy)
- Poor async/await usage for concurrent operations

### Interview Questions

#### Question 1: Foundational - Python Memory Management
**Question:** "Explain how Python manages memory and the implications for training large neural networks. How would you optimize memory usage when working with large datasets?"

**Comprehensive Answer:**
Python uses automatic memory management with reference counting and garbage collection. Every object has a reference count; when it reaches zero, memory is deallocated. However, the garbage collector handles circular references using a generational approach.

For large neural networks:
1. **Memory Implications**: Python objects have significant overhead. A small float takes 28 bytes in Python vs 4 bytes in C
2. **Optimization Strategies**:
   - Use NumPy arrays (contiguous memory, minimal overhead) instead of Python lists
   - Leverage generators and iterators to avoid loading entire datasets into memory
   - Use memory mapping (`numpy.memmap`) for datasets larger than RAM
   - Implement data loaders with proper batching (PyTorch DataLoader, tf.data)
   - Use mixed precision training (FP16) to halve memory requirements
   - Gradient checkpointing to trade computation for memory in transformer models

**Key Points to Cover:**
- Reference counting vs garbage collection
- NumPy's contiguous memory arrays
- Generator expressions and lazy evaluation
- Context managers for resource cleanup (`with` statements)
- Memory profiling tools (memory_profiler, tracemalloc)

**Code Example:**
```python
import numpy as np
from typing import Iterator

# Bad: Loads entire dataset into memory
def load_data_bad(file_path: str) -> list:
    return [process_sample(line) for line in open(file_path)]

# Good: Generator for memory-efficient loading
def load_data_good(file_path: str) -> Iterator:
    with open(file_path) as f:
        for line in f:
            yield process_sample(line)

# Excellent: Using NumPy memmap for huge datasets
def load_large_embeddings(file_path: str) -> np.ndarray:
    # Memory-mapped array, doesn't load into RAM until accessed
    return np.memmap(file_path, dtype='float32', mode='r',
                     shape=(1000000, 768))

# Memory-efficient batch processing
def batch_generator(data_generator: Iterator, batch_size: int):
    batch = []
    for item in data_generator:
        batch.append(item)
        if len(batch) == batch_size:
            yield np.array(batch)
            batch = []
    if batch:  # Don't forget the last partial batch
        yield np.array(batch)
```

**Common Mistakes:**
- Not closing file handles (memory leaks)
- Creating large intermediate lists when generators would suffice
- Not understanding when Python copies vs references objects
- Loading entire datasets with `.readlines()` instead of iterating

**Excellence Indicators:**
- Mentions specific memory profiling tools
- Discusses trade-offs between memory and compute
- Provides concrete examples from experience (e.g., "In my last project, we reduced memory from 64GB to 16GB by...")
- Understands when to use different data structures (lists vs arrays vs tensors)

---

#### Question 2: Intermediate - Async/Concurrent Programming for AI
**Question:** "You're building a chatbot that needs to call multiple LLM APIs in parallel, query a vector database, and log to a monitoring service. How would you implement this efficiently in Python?"

**Comprehensive Answer:**
This requires understanding Python's concurrency models: multithreading (I/O-bound), multiprocessing (CPU-bound), and async/await (I/O-bound, single-threaded).

For this scenario (I/O-bound operations), `asyncio` is ideal:

```python
import asyncio
import aiohttp
from typing import List, Dict
import numpy as np

class ChatbotOrchestrator:
    def __init__(self, openai_key: str, anthropic_key: str):
        self.openai_key = openai_key
        self.anthropic_key = anthropic_key

    async def call_openai(self, prompt: str) -> str:
        """Async call to OpenAI API"""
        async with aiohttp.ClientSession() as session:
            async with session.post(
                'https://api.openai.com/v1/chat/completions',
                headers={'Authorization': f'Bearer {self.openai_key}'},
                json={'model': 'gpt-4', 'messages': [{'role': 'user', 'content': prompt}]}
            ) as resp:
                result = await resp.json()
                return result['choices'][0]['message']['content']

    async def call_anthropic(self, prompt: str) -> str:
        """Async call to Claude API"""
        async with aiohttp.ClientSession() as session:
            async with session.post(
                'https://api.anthropic.com/v1/messages',
                headers={
                    'x-api-key': self.anthropic_key,
                    'anthropic-version': '2023-06-01'
                },
                json={'model': 'claude-3-opus-20240229',
                      'messages': [{'role': 'user', 'content': prompt}],
                      'max_tokens': 1024}
            ) as resp:
                result = await resp.json()
                return result['content'][0]['text']

    async def query_vector_db(self, query_embedding: np.ndarray) -> List[Dict]:
        """Async vector database query"""
        # Simulate async DB call
        await asyncio.sleep(0.1)
        return [{'text': 'relevant doc 1', 'score': 0.89}]

    async def log_interaction(self, user_query: str, response: str):
        """Async logging to monitoring service"""
        async with aiohttp.ClientSession() as session:
            await session.post('https://monitoring-service.com/log',
                             json={'query': user_query, 'response': response})

    async def process_query(self, user_query: str, query_embedding: np.ndarray) -> Dict:
        """
        Orchestrate parallel LLM calls, vector search, and logging
        """
        # Run LLM calls and vector search in parallel
        llm_results, vector_results = await asyncio.gather(
            asyncio.gather(
                self.call_openai(user_query),
                self.call_anthropic(user_query)
            ),
            self.query_vector_db(query_embedding),
            return_exceptions=True  # Don't fail everything if one fails
        )

        # Process results
        openai_response, claude_response = llm_results

        # Use best response (or ensemble)
        final_response = self.select_best_response(
            openai_response, claude_response, vector_results
        )

        # Log asynchronously (don't wait for it)
        asyncio.create_task(self.log_interaction(user_query, final_response))

        return {
            'response': final_response,
            'sources': vector_results,
            'llm_responses': {
                'openai': openai_response,
                'claude': claude_response
            }
        }

    def select_best_response(self, openai_resp: str, claude_resp: str,
                            vector_results: List[Dict]) -> str:
        # Implement selection logic (e.g., based on confidence, length, etc.)
        return openai_resp  # Simplified

# Usage
async def main():
    bot = ChatbotOrchestrator(openai_key='...', anthropic_key='...')
    query_embedding = np.random.rand(768)  # From embedding model

    result = await bot.process_query(
        "What is agentic AI?",
        query_embedding
    )
    print(result['response'])

# Run the async event loop
asyncio.run(main())
```

**Key Points to Cover:**
- `asyncio.gather()` for parallel execution of coroutines
- `aiohttp` for async HTTP requests (not `requests`)
- Error handling with `return_exceptions=True`
- Fire-and-forget tasks with `asyncio.create_task()`
- When to use async vs threading vs multiprocessing

**When to Use Each:**
- **asyncio**: I/O-bound operations (API calls, DB queries) - our scenario
- **threading**: I/O-bound with libraries lacking async support
- **multiprocessing**: CPU-bound operations (model inference without GPU)

**Common Mistakes:**
- Mixing blocking and non-blocking code (using `requests` instead of `aiohttp`)
- Not handling exceptions in concurrent operations
- Awaiting tasks sequentially instead of in parallel
- Creating too many concurrent connections (need connection pooling)

**Excellence Indicators:**
- Discusses connection pooling and rate limiting
- Mentions timeout handling and retry logic
- Knows when async is inappropriate (CPU-bound tasks)
- Can explain event loop internals

---

#### Question 3: Advanced - Decorators and Metaprogramming
**Question:** "Design a decorator that caches LLM API responses with TTL (time-to-live), handles rate limiting, and logs all calls. Show how it would be used in production."

**Comprehensive Answer:**
This tests advanced Python features critical for production AI systems: decorators, caching, and API management.

```python
import time
import functools
import hashlib
import json
from typing import Callable, Any, Optional
from collections import defaultdict
import threading
import logging

logger = logging.getLogger(__name__)

class LLMCallCache:
    """Thread-safe cache with TTL for LLM responses"""
    def __init__(self):
        self._cache = {}
        self._timestamps = {}
        self._lock = threading.Lock()

    def get(self, key: str, ttl: int) -> Optional[Any]:
        with self._lock:
            if key in self._cache:
                if time.time() - self._timestamps[key] < ttl:
                    return self._cache[key]
                else:
                    # Expired, remove
                    del self._cache[key]
                    del self._timestamps[key]
            return None

    def set(self, key: str, value: Any):
        with self._lock:
            self._cache[key] = value
            self._timestamps[key] = time.time()

class RateLimiter:
    """Token bucket rate limiter"""
    def __init__(self, rate: int, per: float):
        """
        Args:
            rate: Number of calls allowed
            per: Time period in seconds
        """
        self.rate = rate
        self.per = per
        self.allowance = rate
        self.last_check = time.time()
        self._lock = threading.Lock()

    def allow_request(self) -> bool:
        with self._lock:
            current = time.time()
            time_passed = current - self.last_check
            self.last_check = current

            # Add tokens based on time passed
            self.allowance += time_passed * (self.rate / self.per)
            if self.allowance > self.rate:
                self.allowance = self.rate

            if self.allowance < 1.0:
                return False
            else:
                self.allowance -= 1.0
                return True

# Global instances
_llm_cache = LLMCallCache()
_rate_limiters = defaultdict(lambda: RateLimiter(rate=10, per=60))

def llm_call(
    ttl: int = 3600,  # Cache for 1 hour
    rate_limit: int = 10,  # 10 calls per minute
    log_calls: bool = True,
    retry_on_rate_limit: bool = True
):
    """
    Decorator for LLM API calls with caching, rate limiting, and logging.

    Args:
        ttl: Cache time-to-live in seconds
        rate_limit: Maximum calls per minute
        log_calls: Whether to log all calls
        retry_on_rate_limit: Whether to wait and retry on rate limit
    """
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            # Create cache key from function name and arguments
            cache_key = _create_cache_key(func.__name__, args, kwargs)

            # Check cache first
            cached_result = _llm_cache.get(cache_key, ttl)
            if cached_result is not None:
                if log_calls:
                    logger.info(f"Cache hit for {func.__name__}")
                return cached_result

            # Rate limiting
            limiter = _rate_limiters[func.__name__]
            limiter.rate = rate_limit

            while not limiter.allow_request():
                if retry_on_rate_limit:
                    wait_time = 60.0 / rate_limit
                    logger.warning(f"Rate limit exceeded for {func.__name__}, "
                                 f"waiting {wait_time}s")
                    time.sleep(wait_time)
                else:
                    raise Exception(f"Rate limit exceeded for {func.__name__}")

            # Log the call
            if log_calls:
                logger.info(f"Calling {func.__name__} with args={args}, kwargs={kwargs}")

            # Make the actual call
            start_time = time.time()
            try:
                result = func(*args, **kwargs)

                # Cache the result
                _llm_cache.set(cache_key, result)

                if log_calls:
                    duration = time.time() - start_time
                    logger.info(f"{func.__name__} completed in {duration:.2f}s")

                return result

            except Exception as e:
                logger.error(f"Error in {func.__name__}: {str(e)}")
                raise

        return wrapper
    return decorator

def _create_cache_key(func_name: str, args: tuple, kwargs: dict) -> str:
    """Create a hash key from function arguments"""
    # Convert args and kwargs to a stable string representation
    key_data = {
        'function': func_name,
        'args': args,
        'kwargs': kwargs
    }
    key_string = json.dumps(key_data, sort_keys=True, default=str)
    return hashlib.md5(key_string.encode()).hexdigest()

# Usage example
@llm_call(ttl=7200, rate_limit=20, log_calls=True)
def call_openai_gpt4(prompt: str, temperature: float = 0.7) -> str:
    """Call OpenAI GPT-4 API"""
    # Actual API call here
    import openai
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        temperature=temperature
    )
    return response.choices[0].message.content

@llm_call(ttl=3600, rate_limit=15)
def call_anthropic_claude(prompt: str, model: str = "claude-3-opus-20240229") -> str:
    """Call Anthropic Claude API"""
    import anthropic
    client = anthropic.Anthropic()
    message = client.messages.create(
        model=model,
        max_tokens=1024,
        messages=[{"role": "user", "content": prompt}]
    )
    return message.content[0].text

# Production usage
if __name__ == "__main__":
    # First call - hits API, caches result
    response1 = call_openai_gpt4("Explain quantum computing")

    # Second call with same args - returns cached result
    response2 = call_openai_gpt4("Explain quantum computing")

    # Different args - hits API again
    response3 = call_openai_gpt4("Explain quantum computing", temperature=0.5)
```

**Key Points to Cover:**
- Decorator syntax and `functools.wraps`
- Thread-safe caching with locks
- Rate limiting algorithms (token bucket)
- Hashing function arguments for cache keys
- Production concerns: logging, monitoring, error handling

**Common Mistakes:**
- Not using `functools.wraps` (loses function metadata)
- Cache keys that aren't deterministic
- Not considering thread safety
- Rate limiters that don't handle bursts properly

**Excellence Indicators:**
- Discusses alternative rate limiting algorithms (leaky bucket, sliding window)
- Mentions distributed caching (Redis) for multi-server deployments
- Considers cache invalidation strategies
- Knows when decorators add too much complexity

---

#### Question 4: Intermediate - Type Hints and Production Code Quality
**Question:** "Why are type hints important in production AI code? Refactor this function to use proper type hints and explain how you'd integrate type checking into your CI/CD pipeline."

**Comprehensive Answer:**
Type hints are critical in production AI systems for several reasons:
1. **Early error detection**: Catch type errors before runtime
2. **Better IDE support**: Autocomplete and refactoring
3. **Documentation**: Types serve as inline documentation
4. **Large team collaboration**: Prevents type-related bugs in complex codebases

```python
from typing import List, Dict, Optional, Union, Tuple, Callable, TypeVar, Generic
from dataclasses import dataclass
from enum import Enum
import numpy as np
import numpy.typing as npt

# Type aliases for clarity
EmbeddingVector = npt.NDArray[np.float32]
TokenIDs = List[int]

class ModelType(Enum):
    GPT4 = "gpt-4"
    CLAUDE3 = "claude-3-opus-20240229"
    GEMINI = "gemini-pro"

@dataclass
class LLMResponse:
    """Structured LLM response"""
    content: str
    model: ModelType
    tokens_used: int
    latency_ms: float
    metadata: Dict[str, Union[str, int, float]]

@dataclass
class RAGContext:
    """Retrieved context for RAG"""
    documents: List[str]
    scores: List[float]
    embeddings: Optional[List[EmbeddingVector]] = None

# Generic type for flexibility
T = TypeVar('T')

class VectorDatabase(Generic[T]):
    """Generic vector database interface"""

    def __init__(self, dimension: int):
        self.dimension = dimension
        self._vectors: List[EmbeddingVector] = []
        self._metadata: List[T] = []

    def add(self, vector: EmbeddingVector, metadata: T) -> None:
        """Add vector with associated metadata"""
        if vector.shape != (self.dimension,):
            raise ValueError(f"Expected {self.dimension}-dim vector, "
                           f"got {vector.shape}")
        self._vectors.append(vector)
        self._metadata.append(metadata)

    def search(
        self,
        query: EmbeddingVector,
        k: int = 5,
        filter_fn: Optional[Callable[[T], bool]] = None
    ) -> Tuple[List[T], List[float]]:
        """
        Search for k nearest vectors

        Args:
            query: Query embedding vector
            k: Number of results to return
            filter_fn: Optional function to filter results

        Returns:
            Tuple of (metadata_list, scores)
        """
        # Implementation details...
        return self._metadata[:k], [0.9, 0.8, 0.7, 0.6, 0.5]

class RAGPipeline:
    """Production RAG pipeline with proper typing"""

    def __init__(
        self,
        vector_db: VectorDatabase[Dict[str, str]],
        embedding_model: Callable[[str], EmbeddingVector],
        llm_client: Callable[[str, ModelType], LLMResponse]
    ):
        self.vector_db = vector_db
        self.embedding_model = embedding_model
        self.llm_client = llm_client

    def retrieve_context(
        self,
        query: str,
        k: int = 5,
        score_threshold: float = 0.7
    ) -> RAGContext:
        """
        Retrieve relevant context for query

        Args:
            query: User query string
            k: Number of documents to retrieve
            score_threshold: Minimum relevance score

        Returns:
            RAGContext with retrieved documents and scores
        """
        # Get query embedding
        query_embedding = self.embedding_model(query)

        # Search vector database
        metadata_list, scores = self.vector_db.search(query_embedding, k=k)

        # Filter by threshold
        filtered_docs = []
        filtered_scores = []
        for meta, score in zip(metadata_list, scores):
            if score >= score_threshold:
                filtered_docs.append(meta['text'])
                filtered_scores.append(score)

        return RAGContext(
            documents=filtered_docs,
            scores=filtered_scores
        )

    def generate_response(
        self,
        query: str,
        context: RAGContext,
        model: ModelType = ModelType.GPT4,
        temperature: float = 0.7
    ) -> LLMResponse:
        """
        Generate response using retrieved context

        Args:
            query: User query
            context: Retrieved RAG context
            model: LLM model to use
            temperature: Sampling temperature

        Returns:
            Structured LLM response
        """
        # Build prompt with context
        context_str = "\n\n".join(context.documents)
        prompt = f"""Context:
{context_str}

Question: {query}

Answer based on the context above:"""

        # Call LLM
        return self.llm_client(prompt, model)

    def query(
        self,
        user_query: str,
        k: int = 5,
        model: ModelType = ModelType.GPT4
    ) -> Dict[str, Union[str, List[str], List[float]]]:
        """
        End-to-end RAG query

        Args:
            user_query: User's question
            k: Number of context documents
            model: LLM model to use

        Returns:
            Dictionary with response and metadata
        """
        # Retrieve context
        context = self.retrieve_context(user_query, k=k)

        # Generate response
        response = self.generate_response(user_query, context, model)

        return {
            'answer': response.content,
            'sources': context.documents,
            'scores': context.scores,
            'model': response.model.value,
            'tokens': response.tokens_used
        }

# CI/CD Integration
"""
pyproject.toml or setup.py:

[tool.mypy]
python_version = "3.11"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
strict_optional = true

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]

.github/workflows/ci.yml:

name: Type Check and Test
on: [push, pull_request]

jobs:
  type-check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: |
          pip install mypy pytest
          pip install -r requirements.txt
      - name: Run mypy
        run: mypy src/ --strict
      - name: Run tests
        run: pytest tests/
"""
```

**Key Points to Cover:**
- Type aliases for complex types (`EmbeddingVector`)
- `dataclass` for structured data
- Generic types with `TypeVar` and `Generic`
- `Optional`, `Union`, `Callable` type hints
- NumPy type hints (`numpy.typing`)
- Integration with mypy in CI/CD

**Common Mistakes:**
- Using `Any` everywhere (defeats the purpose)
- Not typing return values
- Ignoring mypy errors instead of fixing them
- Not using dataclasses for structured data

**Excellence Indicators:**
- Knows about Protocol and structural subtyping
- Discusses trade-offs of strict typing
- Mentions runtime type checking (pydantic)
- Uses type hints to catch actual bugs

---

#### Question 5: Advanced - Python Performance Optimization
**Question:** "Your embeddings generation pipeline is processing 1M documents but taking 10 hours. The bottleneck is Python overhead. How would you optimize this without switching languages?"

**Comprehensive Answer:**
This requires deep understanding of Python performance optimization techniques:

```python
import numpy as np
import torch
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
from typing import List, Iterator
import multiprocessing as mp
from functools import partial

# ===== APPROACH 1: Vectorization with NumPy =====
def embed_slow(texts: List[str], model) -> np.ndarray:
    """Slow: Process one at a time"""
    embeddings = []
    for text in texts:
        emb = model.encode(text)  # Individual encoding
        embeddings.append(emb)
    return np.array(embeddings)

def embed_fast(texts: List[str], model, batch_size: int = 32) -> np.ndarray:
    """Fast: Batch processing"""
    # Process in batches to leverage GPU/vectorization
    all_embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        # Model processes entire batch at once (GPU parallel)
        embeddings = model.encode(batch)
        all_embeddings.append(embeddings)
    return np.vstack(all_embeddings)

# ===== APPROACH 2: Multiprocessing for CPU-bound tasks =====
def process_chunk(texts_chunk: List[str], model_path: str) -> np.ndarray:
    """Process a chunk in a separate process"""
    # Load model in worker process (avoid pickling issues)
    from sentence_transformers import SentenceTransformer
    model = SentenceTransformer(model_path)
    return model.encode(texts_chunk, batch_size=32)

def embed_multiprocess(
    texts: List[str],
    model_path: str,
    num_workers: int = None
) -> np.ndarray:
    """
    Parallel processing across CPU cores

    10x speedup on 10-core machine for CPU-bound tasks
    """
    if num_workers is None:
        num_workers = mp.cpu_count()

    # Split texts into chunks
    chunk_size = len(texts) // num_workers
    chunks = [texts[i:i+chunk_size]
              for i in range(0, len(texts), chunk_size)]

    # Process chunks in parallel
    with ProcessPoolExecutor(max_workers=num_workers) as executor:
        process_fn = partial(process_chunk, model_path=model_path)
        results = list(executor.map(process_fn, chunks))

    return np.vstack(results)

# ===== APPROACH 3: GPU Optimization with PyTorch =====
def embed_gpu_optimized(
    texts: List[str],
    model,
    batch_size: int = 64,
    device: str = 'cuda'
) -> np.ndarray:
    """
    GPU-optimized embedding generation

    - Large batch sizes for GPU efficiency
    - Mixed precision (FP16) for 2x speedup
    - Gradient disabled for inference
    """
    import torch
    from torch.cuda.amp import autocast

    model = model.to(device)
    model.eval()

    all_embeddings = []

    with torch.no_grad():  # Disable gradient computation
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]

            # Tokenize
            inputs = model.tokenize(batch)
            inputs = {k: v.to(device) for k, v in inputs.items()}

            # Mixed precision inference
            with autocast():  # FP16 for speed
                outputs = model(**inputs)
                embeddings = outputs.last_hidden_state.mean(dim=1)

            # Move to CPU and convert to numpy
            all_embeddings.append(embeddings.cpu().numpy())

    return np.vstack(all_embeddings)

# ===== APPROACH 4: Streaming and Memory Mapping =====
class StreamingEmbedder:
    """
    Process embeddings without loading all into memory
    Crucial for 1M+ documents
    """
    def __init__(self, model, output_path: str, embedding_dim: int):
        self.model = model
        self.output_path = output_path
        self.embedding_dim = embedding_dim

    def embed_stream(
        self,
        text_iterator: Iterator[str],
        total_docs: int,
        batch_size: int = 32
    ) -> None:
        """
        Stream process and write directly to disk

        Memory usage: O(batch_size) instead of O(total_docs)
        """
        # Create memory-mapped file for output
        embeddings_mmap = np.memmap(
            self.output_path,
            dtype='float32',
            mode='w+',
            shape=(total_docs, self.embedding_dim)
        )

        batch = []
        current_idx = 0

        for text in text_iterator:
            batch.append(text)

            if len(batch) == batch_size:
                # Process batch
                embs = self.model.encode(batch)

                # Write directly to memory-mapped file
                embeddings_mmap[current_idx:current_idx+batch_size] = embs

                current_idx += batch_size
                batch = []

        # Process final partial batch
        if batch:
            embs = self.model.encode(batch)
            embeddings_mmap[current_idx:current_idx+len(batch)] = embs

        # Flush to disk
        embeddings_mmap.flush()

# ===== APPROACH 5: Cython for Critical Loops =====
"""
# embeddings_utils.pyx (Cython file)
import numpy as np
cimport numpy as np
cimport cython

@cython.boundscheck(False)  # Disable bounds checking
@cython.wraparound(False)   # Disable negative indexing
def cosine_similarity_fast(
    np.ndarray[np.float32_t, ndim=2] embeddings,
    np.ndarray[np.float32_t, ndim=1] query
) -> np.ndarray[np.float32_t, ndim=1]:
    '''
    Compute cosine similarity 10x faster than pure Python
    '''
    cdef int n = embeddings.shape[0]
    cdef int dim = embeddings.shape[1]
    cdef np.ndarray[np.float32_t, ndim=1] scores = np.zeros(n, dtype=np.float32)
    cdef float query_norm = 0.0
    cdef float emb_norm = 0.0
    cdef float dot = 0.0
    cdef int i, j

    # Compute query norm
    for j in range(dim):
        query_norm += query[j] * query[j]
    query_norm = query_norm ** 0.5

    # Compute similarities
    for i in range(n):
        dot = 0.0
        emb_norm = 0.0
        for j in range(dim):
            dot += embeddings[i, j] * query[j]
            emb_norm += embeddings[i, j] * embeddings[i, j]
        emb_norm = emb_norm ** 0.5
        scores[i] = dot / (emb_norm * query_norm)

    return scores
"""

# ===== COMPLETE PRODUCTION PIPELINE =====
class OptimizedEmbeddingPipeline:
    """
    Complete optimized pipeline combining all techniques

    Expected speedup: 50-100x vs naive implementation
    """
    def __init__(self, model_path: str, device: str = 'cuda'):
        from sentence_transformers import SentenceTransformer
        self.model = SentenceTransformer(model_path)
        self.model.to(device)
        self.device = device

    def process_million_documents(
        self,
        input_file: str,
        output_file: str,
        batch_size: int = 128,  # Large for GPU
        num_workers: int = 4     # For I/O parallelism
    ):
        """
        Process 1M documents efficiently

        Optimizations:
        - Streaming (constant memory)
        - GPU batching
        - Mixed precision
        - Parallel I/O
        - Memory mapping
        """
        import torch
        from torch.cuda.amp import autocast

        # Count total documents first (for memory mapping)
        total_docs = sum(1 for _ in open(input_file))
        embedding_dim = self.model.get_sentence_embedding_dimension()

        # Create memory-mapped output
        embeddings_mmap = np.memmap(
            output_file,
            dtype='float32',
            mode='w+',
            shape=(total_docs, embedding_dim)
        )

        # Stream process
        current_idx = 0
        batch_texts = []

        self.model.eval()
        with torch.no_grad():
            with open(input_file, 'r') as f:
                for line in f:
                    batch_texts.append(line.strip())

                    if len(batch_texts) == batch_size:
                        # Process batch with GPU + FP16
                        with autocast():
                            embs = self.model.encode(
                                batch_texts,
                                convert_to_numpy=True,
                                device=self.device,
                                show_progress_bar=False
                            )

                        # Write to memory-mapped file
                        embeddings_mmap[current_idx:current_idx+batch_size] = embs
                        current_idx += batch_size
                        batch_texts = []

                        # Progress
                        if current_idx % 10000 == 0:
                            print(f"Processed {current_idx}/{total_docs}")

            # Final batch
            if batch_texts:
                embs = self.model.encode(batch_texts, convert_to_numpy=True)
                embeddings_mmap[current_idx:] = embs

        embeddings_mmap.flush()
        print(f"Completed: {total_docs} embeddings saved to {output_file}")

# Benchmarking
if __name__ == "__main__":
    """
    Naive: 10 hours for 1M docs
    Optimized: ~10 minutes for 1M docs

    Speedup: 60x
    """
    pipeline = OptimizedEmbeddingPipeline(
        'sentence-transformers/all-MiniLM-L6-v2',
        device='cuda'
    )

    pipeline.process_million_documents(
        'documents_1m.txt',
        'embeddings_1m.npy',
        batch_size=256
    )
```

**Key Points to Cover:**
- **Vectorization**: NumPy/GPU batch processing
- **Multiprocessing**: Parallel CPU utilization
- **GPU optimization**: Large batches, mixed precision, no gradients
- **Memory management**: Streaming, memory mapping
- **Cython**: For critical numerical loops

**Optimization Checklist:**
1. Profile first (`cProfile`, `line_profiler`)
2. Vectorize with NumPy
3. Use GPU for batch operations
4. Multiprocessing for CPU-bound
5. Streaming for large datasets
6. Cython/Numba for hot loops

**Common Mistakes:**
- Optimizing without profiling (premature optimization)
- Using threading for CPU-bound tasks (GIL limitation)
- Loading entire dataset into memory
- Small batch sizes on GPU (underutilization)

**Excellence Indicators:**
- Provides benchmarking results
- Discusses trade-offs (memory vs speed)
- Knows when each optimization applies
- Mentions profiling tools

**Follow-up Discussion:**
"How would you handle embeddings that don't fit in GPU memory?"
- Batch processing with CPUâ†’GPU transfers
- Model parallelism (split model across GPUs)
- Gradient checkpointing
- Quantization (int8) to reduce memory

---

## 1.2 Algorithms & Data Structures

### Concept Definition
Algorithms and data structures form the foundation of efficient AI systems. While AI engineers focus on models, understanding computational complexity, graph algorithms, search algorithms, and optimal data structures is crucial for building scalable AI applications, especially for agent systems, knowledge graphs, and optimization problems.

### Rationale & Industry Relevance
- **Agent Planning**: Search algorithms (A*, Dijkstra) used in agent path planning and decision-making
- **Graph Structures**: Knowledge graphs, RAG systems, multi-agent communication use graph algorithms
- **Optimization**: Training algorithms, hyperparameter search require understanding of optimization techniques
- **System Scale**: Poor algorithmic choices lead to exponential slowdowns at scale

### Interview Questions

#### Question 6: Intermediate - Graph Algorithms for Knowledge Graphs
**Question:** "You're building a knowledge graph for an AI agent. Implement an algorithm to find the shortest path between two entities, and explain how you'd optimize it for a graph with 10 million nodes."

**Comprehensive Answer:**

```python
from typing import Dict, List, Set, Tuple, Optional
from heapq import heappush, heappop
from collections import defaultdict, deque
import numpy as np

class KnowledgeGraph:
    """
    Knowledge graph for AI agent reasoning

    Supports:
    - Entity relationships
    - Weighted edges (relationship strength)
    - Bidirectional search
    - Caching for frequent queries
    """
    def __init__(self):
        # Adjacency list: entity -> [(neighbor, weight, relationship)]
        self.graph: Dict[str, List[Tuple[str, float, str]]] = defaultdict(list)
        self.entities: Set[str] = set()
        # Cache for frequently queried paths
        self._path_cache: Dict[Tuple[str, str], List[str]] = {}

    def add_edge(self, entity1: str, entity2: str,
                 relationship: str, weight: float = 1.0,
                 bidirectional: bool = True):
        """Add relationship between entities"""
        self.entities.add(entity1)
        self.entities.add(entity2)
        self.graph[entity1].append((entity2, weight, relationship))
        if bidirectional:
            self.graph[entity2].append((entity1, weight, relationship))

    def dijkstra_shortest_path(
        self,
        start: str,
        end: str
    ) -> Optional[Tuple[List[str], float]]:
        """
        Find shortest path using Dijkstra's algorithm

        Time: O((V + E) log V) with binary heap
        Space: O(V)

        Returns:
            (path, total_cost) or None if no path exists
        """
        if start not in self.entities or end not in self.entities:
            return None

        # Priority queue: (cost, node, path)
        pq = [(0, start, [start])]
        visited = set()

        while pq:
            cost, node, path = heappop(pq)

            if node in visited:
                continue

            if node == end:
                return (path, cost)

            visited.add(node)

            for neighbor, weight, _ in self.graph[node]:
                if neighbor not in visited:
                    new_cost = cost + weight
                    new_path = path + [neighbor]
                    heappush(pq, (new_cost, neighbor, new_path))

        return None  # No path found

    def bidirectional_search(
        self,
        start: str,
        end: str
    ) -> Optional[List[str]]:
        """
        Bidirectional BFS for unweighted graphs

        Time: O(b^(d/2)) vs O(b^d) for unidirectional
        Where b=branching factor, d=depth

        ~100x faster for large graphs
        """
        if start == end:
            return [start]

        # Search from both ends
        forward_queue = deque([(start, [start])])
        backward_queue = deque([(end, [end])])

        forward_visited = {start: [start]}
        backward_visited = {end: [end]}

        while forward_queue and backward_queue:
            # Expand forward
            if forward_queue:
                node, path = forward_queue.popleft()
                for neighbor, _, _ in self.graph[node]:
                    if neighbor in backward_visited:
                        # Paths meet!
                        backward_path = backward_visited[neighbor]
                        return path + backward_path[::-1][1:]

                    if neighbor not in forward_visited:
                        new_path = path + [neighbor]
                        forward_visited[neighbor] = new_path
                        forward_queue.append((neighbor, new_path))

            # Expand backward
            if backward_queue:
                node, path = backward_queue.popleft()
                for neighbor, _, _ in self.graph[node]:
                    if neighbor in forward_visited:
                        # Paths meet!
                        forward_path = forward_visited[neighbor]
                        return forward_path + path[::-1][1:]

                    if neighbor not in backward_visited:
                        new_path = path + [neighbor]
                        backward_visited[neighbor] = new_path
                        backward_queue.append((neighbor, new_path))

        return None

    def a_star_search(
        self,
        start: str,
        end: str,
        heuristic_fn: Optional[callable] = None
    ) -> Optional[Tuple[List[str], float]]:
        """
        A* search with heuristic for faster pathfinding

        Time: O(E log V) - often much better with good heuristic

        For knowledge graphs, heuristic could be:
        - Embedding similarity between entities
        - Number of shared relationships
        """
        if heuristic_fn is None:
            # Default: BFS (h=0)
            heuristic_fn = lambda x, y: 0

        # Priority queue: (f_score, node, g_score, path)
        # f_score = g_score + h_score
        h_start = heuristic_fn(start, end)
        pq = [(h_start, start, 0, [start])]
        visited = set()

        while pq:
            f_score, node, g_score, path = heappop(pq)

            if node in visited:
                continue

            if node == end:
                return (path, g_score)

            visited.add(node)

            for neighbor, weight, _ in self.graph[node]:
                if neighbor not in visited:
                    new_g = g_score + weight
                    new_h = heuristic_fn(neighbor, end)
                    new_f = new_g + new_h
                    new_path = path + [neighbor]
                    heappush(pq, (new_f, neighbor, new_g, new_path))

        return None

    def find_path_cached(self, start: str, end: str) -> Optional[List[str]]:
        """
        Cached path finding for frequent queries

        Critical for production: ~90% of queries are repeated
        """
        # Check cache
        cache_key = (start, end)
        if cache_key in self._path_cache:
            return self._path_cache[cache_key]

        # Compute path
        result = self.bidirectional_search(start, end)

        # Cache result
        if result:
            self._path_cache[cache_key] = result

        return result

# ===== OPTIMIZATIONS FOR 10M NODE GRAPH =====

class ScalableKnowledgeGraph:
    """
    Optimizations for massive graphs (10M+ nodes)

    Techniques:
    1. Graph partitioning
    2. Hierarchical indexing
    3. Approximate algorithms
    4. Distributed graph storage
    """
    def __init__(self, num_partitions: int = 100):
        self.num_partitions = num_partitions
        # Partition graphs by entity ID hash
        self.partitions: List[KnowledgeGraph] = [
            KnowledgeGraph() for _ in range(num_partitions)
        ]
        # Entity to partition mapping
        self.entity_partition: Dict[str, int] = {}

    def _get_partition(self, entity: str) -> int:
        """Hash entity to partition"""
        if entity not in self.entity_partition:
            partition_id = hash(entity) % self.num_partitions
            self.entity_partition[entity] = partition_id
        return self.entity_partition[entity]

    def add_edge(self, entity1: str, entity2: str,
                 relationship: str, weight: float = 1.0):
        """Add edge to appropriate partition(s)"""
        p1 = self._get_partition(entity1)
        p2 = self._get_partition(entity2)

        # Add to entity1's partition
        self.partitions[p1].add_edge(entity1, entity2, relationship, weight)

        # If cross-partition, add to entity2's partition too
        if p1 != p2:
            self.partitions[p2].add_edge(entity1, entity2, relationship, weight)

    def shortest_path_partitioned(
        self,
        start: str,
        end: str
    ) -> Optional[List[str]]:
        """
        Find shortest path across partitions

        For same partition: Fast local search
        For cross-partition: Use partition boundaries
        """
        p_start = self._get_partition(start)
        p_end = self._get_partition(end)

        if p_start == p_end:
            # Same partition - fast local search
            return self.partitions[p_start].bidirectional_search(start, end)
        else:
            # Cross-partition - more complex
            # In production: Use graph database (Neo4j, Neptune)
            return self._cross_partition_search(start, end)

    def _cross_partition_search(
        self,
        start: str,
        end: str
    ) -> Optional[List[str]]:
        """
        Search across partitions

        Strategy:
        1. Find boundary nodes (entities with cross-partition edges)
        2. Search start -> boundary, boundary -> end
        3. Combine paths
        """
        # Simplified implementation
        # Production: Use distributed graph algorithm
        pass

# ===== EMBEDDING-BASED HEURISTIC FOR A* =====

class EmbeddingKnowledgeGraph(KnowledgeGraph):
    """
    Knowledge graph with entity embeddings for intelligent search
    """
    def __init__(self, embedding_dim: int = 768):
        super().__init__()
        # Entity embeddings for semantic similarity
        self.embeddings: Dict[str, np.ndarray] = {}
        self.embedding_dim = embedding_dim

    def add_entity_embedding(self, entity: str, embedding: np.ndarray):
        """Add semantic embedding for entity"""
        self.embeddings[entity] = embedding

    def embedding_similarity_heuristic(
        self,
        entity1: str,
        entity2: str
    ) -> float:
        """
        Heuristic based on embedding similarity

        Higher similarity = likely closer in graph
        Returns distance estimate (inverse of similarity)
        """
        if entity1 not in self.embeddings or entity2 not in self.embeddings:
            return 0  # No information

        emb1 = self.embeddings[entity1]
        emb2 = self.embeddings[entity2]

        # Cosine similarity
        similarity = np.dot(emb1, emb2) / (
            np.linalg.norm(emb1) * np.linalg.norm(emb2)
        )

        # Convert to distance (lower = closer)
        distance = 1 - similarity
        return distance

    def smart_shortest_path(
        self,
        start: str,
        end: str
    ) -> Optional[Tuple[List[str], float]]:
        """
        A* with embedding-based heuristic

        Typically 5-10x faster than Dijkstra for large graphs
        """
        return self.a_star_search(
            start,
            end,
            heuristic_fn=self.embedding_similarity_heuristic
        )

# Usage example for AI agent reasoning
if __name__ == "__main__":
    # Build knowledge graph
    kg = EmbeddingKnowledgeGraph()

    # Add relationships
    kg.add_edge("Python", "Programming Language", "IS_A")
    kg.add_edge("Python", "AI Development", "USED_FOR")
    kg.add_edge("AI Development", "Machine Learning", "INCLUDES")
    kg.add_edge("Machine Learning", "TensorFlow", "USES_FRAMEWORK")
    kg.add_edge("TensorFlow", "Google", "DEVELOPED_BY")

    # Add entity embeddings (from embedding model)
    kg.add_entity_embedding("Python", np.random.rand(768))
    kg.add_entity_embedding("Google", np.random.rand(768))

    # Find shortest path for agent reasoning
    path, cost = kg.smart_shortest_path("Python", "Google")
    print(f"Path: {' -> '.join(path)}, Cost: {cost}")
```

**Key Points to Cover:**
- **Algorithm choice**: Dijkstra for weighted, BFS for unweighted, A* with heuristic
- **Bidirectional search**: ~100x speedup for large graphs
- **Graph partitioning**: Essential for 10M+ nodes
- **Caching**: 90% of queries are repeated in production
- **Embeddings as heuristic**: Leverage semantic similarity

**Optimization for 10M Nodes:**
1. **Graph database**: Use Neo4j, Amazon Neptune (not in-memory)
2. **Partitioning**: Shard by entity hash
3. **Indexing**: B-trees on entity IDs
4. **Approximate algorithms**: Landmark-based distance estimates
5. **Caching**: Redis for frequent paths

**Common Mistakes:**
- Using naive DFS (exponential time)
- Not considering bidirectional search
- Loading entire graph into memory
- No caching for repeated queries

**Excellence Indicators:**
- Discusses trade-offs of different algorithms
- Mentions real graph databases (Neo4j)
- Knows when approximate is acceptable
- Provides complexity analysis

---

### Summary: Programming & CS Fundamentals

**Critical Takeaways:**
- Python mastery is non-negotiable (71% of jobs)
- Advanced Python: decorators, async, type hints, performance optimization
- Algorithms: graph search, optimization, complexity analysis
- Production mindset: caching, monitoring, error handling

**Study Priority:**
- **High**: Python proficiency, async programming, NumPy
- **Medium**: Advanced decorators, Cython, multiprocessing
- **Lower**: Deep algorithm theory (unless CS-heavy role)

**Preparation Time:**
- Foundational Python: 2 weeks
- Advanced Python: 2-3 weeks
- Algorithms: 1-2 weeks (if CS background)

---

# 2. AI/ML Frameworks & Deep Learning

## 2.1 TensorFlow & PyTorch

### Concept Definition
TensorFlow (Google) and PyTorch (Meta) are the two dominant deep learning frameworks, required in virtually all AI Engineer roles. TensorFlow emphasizes production deployment and mobile/edge support, while PyTorch is preferred for research and rapid prototyping. Modern AI engineers must be proficient in at least one, preferably both.

### Rationale & Industry Relevance
- **Universal Requirement**: 85%+ of AI Engineer postings require TensorFlow and/or PyTorch
- **Production Deployment**: TensorFlow Serving, TorchServe power millions of production models
- **LLM Fine-tuning**: PyTorch dominates for fine-tuning GPT, BERT, LLaMA models
- **Career Impact**: Framework proficiency is the #3 most common requirement after Python and LLMs

### Pros & Cons

**TensorFlow:**
- **Pros**: Production-ready (TF Serving), mobile (TFLite), TPU support, static graph optimization
- **Cons**: Steeper learning curve, less intuitive for research, eager execution added later

**PyTorch:**
- **Pros**: Pythonic, dynamic graphs, excellent for research, dominant in LLM ecosystem
- **Cons**: Production tools less mature (improving with TorchServe), historically slower

**When to Use Each:**
- **TensorFlow**: Mobile apps, edge devices, Google Cloud TPU, large-scale production (legacy systems)
- **PyTorch**: Research, LLM fine-tuning, rapid prototyping, most new AI projects

### Interview Questions

#### Question 7: Foundational - Custom Training Loop
**Question:** "Implement a custom training loop in PyTorch for fine-tuning a BERT model on a sentiment classification task. Include gradient accumulation for large batch sizes."

**Comprehensive Answer:**

```python
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup
from typing import Dict, List, Tuple
import numpy as np
from tqdm import tqdm

class SentimentDataset(Dataset):
    """Custom dataset for sentiment classification"""
    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self) -> int:
        return len(self.texts)

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        text = self.texts[idx]
        label = self.labels[idx]

        # Tokenize
        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0),
            'label': torch.tensor(label, dtype=torch.long)
        }

class BERTSentimentClassifier(nn.Module):
    """BERT-based sentiment classifier"""
    def __init__(self, bert_model_name: str, num_classes: int = 3, dropout: float = 0.1):
        super().__init__()
        self.bert = BertModel.from_pretrained(bert_model_name)
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)

    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
        # Get BERT outputs
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        # Use [CLS] token representation
        pooled_output = outputs.pooler_output  # [batch_size, hidden_size]

        # Dropout and classification
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)  # [batch_size, num_classes]

        return logits

class SentimentTrainer:
    """
    Custom trainer with advanced features:
    - Gradient accumulation for large effective batch sizes
    - Mixed precision training (FP16)
    - Learning rate scheduling with warmup
    - Gradient clipping
    - Checkpointing
    """
    def __init__(
        self,
        model: nn.Module,
        train_dataloader: DataLoader,
        val_dataloader: DataLoader,
        device: str = 'cuda',
        learning_rate: float = 2e-5,
        num_epochs: int = 3,
        accumulation_steps: int = 4,  # Effective batch size = batch_size * accumulation_steps
        max_grad_norm: float = 1.0,
        warmup_steps: int = 100
    ):
        self.model = model.to(device)
        self.train_dataloader = train_dataloader
        self.val_dataloader = val_dataloader
        self.device = device
        self.num_epochs = num_epochs
        self.accumulation_steps = accumulation_steps
        self.max_grad_norm = max_grad_norm

        # Loss function
        self.criterion = nn.CrossEntropyLoss()

        # Optimizer
        self.optimizer = AdamW(
            model.parameters(),
            lr=learning_rate,
            weight_decay=0.01  # L2 regularization
        )

        # Learning rate scheduler with warmup
        total_steps = len(train_dataloader) * num_epochs // accumulation_steps
        self.scheduler = get_linear_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=total_steps
        )

        # Mixed precision training
        self.scaler = torch.cuda.amp.GradScaler()

        # Metrics tracking
        self.train_losses = []
        self.val_losses = []
        self.val_accuracies = []

    def train_epoch(self) -> float:
        """Train for one epoch with gradient accumulation"""
        self.model.train()
        total_loss = 0
        self.optimizer.zero_grad()

        progress_bar = tqdm(enumerate(self.train_dataloader), total=len(self.train_dataloader))

        for step, batch in progress_bar:
            # Move to device
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['label'].to(self.device)

            # Mixed precision forward pass
            with torch.cuda.amp.autocast():
                logits = self.model(input_ids, attention_mask)
                loss = self.criterion(logits, labels)
                # Scale loss for gradient accumulation
                loss = loss / self.accumulation_steps

            # Backward pass with mixed precision
            self.scaler.scale(loss).backward()

            # Gradient accumulation: only step optimizer every N steps
            if (step + 1) % self.accumulation_steps == 0:
                # Gradient clipping (prevent exploding gradients)
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.max_grad_norm
                )

                # Optimizer step
                self.scaler.step(self.optimizer)
                self.scaler.update()
                self.scheduler.step()
                self.optimizer.zero_grad()

            # Track loss
            total_loss += loss.item() * self.accumulation_steps

            # Update progress bar
            progress_bar.set_postfix({
                'loss': loss.item() * self.accumulation_steps,
                'lr': self.scheduler.get_last_lr()[0]
            })

        return total_loss / len(self.train_dataloader)

    @torch.no_grad()
    def validate(self) -> Tuple[float, float]:
        """Validate model"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0

        for batch in tqdm(self.val_dataloader, desc="Validating"):
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['label'].to(self.device)

            # Forward pass
            logits = self.model(input_ids, attention_mask)
            loss = self.criterion(logits, labels)

            # Metrics
            total_loss += loss.item()
            predictions = torch.argmax(logits, dim=1)
            correct += (predictions == labels).sum().item()
            total += labels.size(0)

        avg_loss = total_loss / len(self.val_dataloader)
        accuracy = correct / total

        return avg_loss, accuracy

    def train(self, checkpoint_dir: str = './checkpoints'):
        """Full training loop"""
        import os
        os.makedirs(checkpoint_dir, exist_ok=True)

        best_val_accuracy = 0

        for epoch in range(self.num_epochs):
            print(f"\nEpoch {epoch + 1}/{self.num_epochs}")

            # Train
            train_loss = self.train_epoch()
            self.train_losses.append(train_loss)
            print(f"Train Loss: {train_loss:.4f}")

            # Validate
            val_loss, val_accuracy = self.validate()
            self.val_losses.append(val_loss)
            self.val_accuracies.append(val_accuracy)
            print(f"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}")

            # Save best model
            if val_accuracy > best_val_accuracy:
                best_val_accuracy = val_accuracy
                checkpoint_path = os.path.join(checkpoint_dir, 'best_model.pt')
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'scheduler_state_dict': self.scheduler.state_dict(),
                    'val_accuracy': val_accuracy,
                    'val_loss': val_loss
                }, checkpoint_path)
                print(f"Saved best model with accuracy: {val_accuracy:.4f}")

        print(f"\nTraining complete. Best validation accuracy: {best_val_accuracy:.4f}")

# ===== USAGE EXAMPLE =====
if __name__ == "__main__":
    # Configuration
    BERT_MODEL = 'bert-base-uncased'
    NUM_CLASSES = 3  # positive, neutral, negative
    BATCH_SIZE = 8   # Small batch size
    ACCUMULATION_STEPS = 8  # Effective batch size = 64
    MAX_LENGTH = 128
    NUM_EPOCHS = 3
    LEARNING_RATE = 2e-5
    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

    # Sample data
    train_texts = ["This is great!", "Terrible experience", "It's okay"] * 1000
    train_labels = [2, 0, 1] * 1000  # 2=positive, 1=neutral, 0=negative
    val_texts = ["Amazing!", "Bad", "Average"] * 100
    val_labels = [2, 0, 1] * 100

    # Initialize tokenizer
    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)

    # Create datasets
    train_dataset = SentimentDataset(train_texts, train_labels, tokenizer, MAX_LENGTH)
    val_dataset = SentimentDataset(val_texts, val_labels, tokenizer, MAX_LENGTH)

    # Create dataloaders
    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)

    # Initialize model
    model = BERTSentimentClassifier(BERT_MODEL, NUM_CLASSES)

    # Initialize trainer
    trainer = SentimentTrainer(
        model=model,
        train_dataloader=train_dataloader,
        val_dataloader=val_dataloader,
        device=DEVICE,
        learning_rate=LEARNING_RATE,
        num_epochs=NUM_EPOCHS,
        accumulation_steps=ACCUMULATION_STEPS
    )

    # Train
    trainer.train()
```

**Key Points to Cover:**
- **Gradient accumulation**: Enables large effective batch sizes with limited GPU memory
- **Mixed precision (FP16)**: 2x speedup and 50% memory reduction
- **Gradient clipping**: Prevents exploding gradients
- **Learning rate scheduling**: Warmup + linear decay
- **Checkpointing**: Save best model based on validation metrics

**Why Gradient Accumulation?**
- GPU memory limits batch size (e.g., 8 with 16GB GPU)
- But optimal batch size is often 64-128
- Accumulate gradients over 8 steps â†’ effective batch size of 64
- Trade-off: More training steps, but same effective learning

**Common Mistakes:**
- Forgetting to scale loss by accumulation steps
- Not zeroing gradients after accumulation step
- Using `.backward()` without scaler for mixed precision
- Not moving data to device

**Excellence Indicators:**
- Explains why warmup helps (gradients are noisy early on)
- Discusses batch size vs learning rate relationship
- Mentions alternative optimizers (AdamW vs Adam vs SGD)
- Knows when to use different precision types

**Follow-up**: "How would you implement distributed training across 8 GPUs?"
- `torch.nn.DataParallel` (simple, single-node)
- `torch.nn.parallel.DistributedDataParallel` (better, multi-node)
- `accelerate` library (HuggingFace, simplifies distributed)
- FSDP (Fully Sharded Data Parallel) for very large models

---

#### Question 8: Intermediate - Model Architecture Design
**Question:** "Design a neural architecture for a multi-modal model that takes both text and images as input. Explain your design choices."

**Comprehensive Answer:**

```python
import torch
import torch.nn as nn
from transformers import BertModel, BertTokenizer
from torchvision import models
from typing import Dict, Tuple

class MultiModalFusionModel(nn.Module):
    """
    Multi-modal model: Text + Image â†’ Classification/Generation

    Architecture:
    1. Text encoder (BERT)
    2. Image encoder (ResNet/ViT)
    3. Fusion layer (cross-attention or concatenation)
    4. Task head (classification/generation)

    Design rationale:
    - Pre-trained encoders (transfer learning)
    - Cross-attention for interaction between modalities
    - Flexible task head for different downstream tasks
    """
    def __init__(
        self,
        text_encoder_name: str = 'bert-base-uncased',
        image_encoder_name: str = 'resnet50',
        fusion_type: str = 'cross_attention',  # or 'concatenation'
        num_classes: int = 10,
        hidden_dim: int = 512,
        dropout: float = 0.1
    ):
        super().__init__()

        self.fusion_type = fusion_type

        # ===== TEXT ENCODER =====
        self.text_encoder = BertModel.from_pretrained(text_encoder_name)
        text_hidden_size = self.text_encoder.config.hidden_size  # 768 for BERT-base

        # Option to freeze text encoder (fine-tuning only top layers)
        # self._freeze_encoder(self.text_encoder, freeze_layers=10)

        # ===== IMAGE ENCODER =====
        if image_encoder_name == 'resnet50':
            resnet = models.resnet50(pretrained=True)
            # Remove final classification layer
            self.image_encoder = nn.Sequential(*list(resnet.children())[:-1])
            image_hidden_size = 2048  # ResNet50 output channels
        elif image_encoder_name == 'vit':
            from transformers import ViTModel
            self.image_encoder = ViTModel.from_pretrained('google/vit-base-patch16-224')
            image_hidden_size = self.image_encoder.config.hidden_size

        # ===== PROJECTION LAYERS =====
        # Project both modalities to common dimension
        self.text_projection = nn.Linear(text_hidden_size, hidden_dim)
        self.image_projection = nn.Linear(image_hidden_size, hidden_dim)

        # ===== FUSION LAYER =====
        if fusion_type == 'cross_attention':
            # Cross-attention between text and image
            self.cross_attention = nn.MultiheadAttention(
                embed_dim=hidden_dim,
                num_heads=8,
                dropout=dropout,
                batch_first=True
            )
            fusion_output_dim = hidden_dim
        elif fusion_type == 'concatenation':
            # Simple concatenation + MLP
            self.fusion_mlp = nn.Sequential(
                nn.Linear(hidden_dim * 2, hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(hidden_dim, hidden_dim)
            )
            fusion_output_dim = hidden_dim
        elif fusion_type == 'gated':
            # Gated fusion (learns importance of each modality)
            self.gate = nn.Linear(hidden_dim * 2, 1)
            fusion_output_dim = hidden_dim

        # ===== TASK HEAD =====
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Sequential(
            nn.Linear(fusion_output_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, num_classes)
        )

    def _freeze_encoder(self, encoder, freeze_layers: int):
        """Freeze bottom layers of encoder"""
        for param in encoder.parameters():
            param.requires_grad = False

        # Unfreeze top layers
        for i, layer in enumerate(encoder.encoder.layer):
            if i >= freeze_layers:
                for param in layer.parameters():
                    param.requires_grad = True

    def encode_text(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
        """
        Encode text with BERT

        Args:
            input_ids: [batch_size, seq_len]
            attention_mask: [batch_size, seq_len]

        Returns:
            text_features: [batch_size, hidden_dim]
        """
        outputs = self.text_encoder(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        # Use [CLS] token or mean pooling
        pooled_output = outputs.pooler_output  # [batch_size, 768]
        text_features = self.text_projection(pooled_output)  # [batch_size, hidden_dim]
        return text_features

    def encode_image(self, images: torch.Tensor) -> torch.Tensor:
        """
        Encode images with ResNet/ViT

        Args:
            images: [batch_size, 3, 224, 224]

        Returns:
            image_features: [batch_size, hidden_dim]
        """
        if isinstance(self.image_encoder, nn.Sequential):
            # ResNet
            features = self.image_encoder(images)  # [batch_size, 2048, 1, 1]
            features = features.squeeze(-1).squeeze(-1)  # [batch_size, 2048]
        else:
            # ViT
            features = self.image_encoder(images).last_hidden_state[:, 0]  # [CLS] token

        image_features = self.image_projection(features)  # [batch_size, hidden_dim]
        return image_features

    def fuse_modalities(
        self,
        text_features: torch.Tensor,
        image_features: torch.Tensor
    ) -> torch.Tensor:
        """
        Fuse text and image features

        Args:
            text_features: [batch_size, hidden_dim]
            image_features: [batch_size, hidden_dim]

        Returns:
            fused_features: [batch_size, hidden_dim]
        """
        if self.fusion_type == 'cross_attention':
            # Text attends to image
            # Need to unsqueeze to add sequence dimension for attention
            text_seq = text_features.unsqueeze(1)  # [batch_size, 1, hidden_dim]
            image_seq = image_features.unsqueeze(1)  # [batch_size, 1, hidden_dim]

            # Cross-attention: text queries image
            attn_output, _ = self.cross_attention(
                query=text_seq,
                key=image_seq,
                value=image_seq
            )
            fused_features = attn_output.squeeze(1)  # [batch_size, hidden_dim]

        elif self.fusion_type == 'concatenation':
            # Simple concatenation
            concat_features = torch.cat([text_features, image_features], dim=1)
            fused_features = self.fusion_mlp(concat_features)

        elif self.fusion_type == 'gated':
            # Gated fusion (adaptive weighting)
            concat_features = torch.cat([text_features, image_features], dim=1)
            gate_weight = torch.sigmoid(self.gate(concat_features))  # [batch_size, 1]
            fused_features = gate_weight * text_features + (1 - gate_weight) * image_features

        return fused_features

    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        images: torch.Tensor
    ) -> torch.Tensor:
        """
        Forward pass

        Args:
            input_ids: [batch_size, seq_len]
            attention_mask: [batch_size, seq_len]
            images: [batch_size, 3, 224, 224]

        Returns:
            logits: [batch_size, num_classes]
        """
        # Encode both modalities
        text_features = self.encode_text(input_ids, attention_mask)
        image_features = self.encode_image(images)

        # Fuse modalities
        fused_features = self.fuse_modalities(text_features, image_features)

        # Classification
        fused_features = self.dropout(fused_features)
        logits = self.classifier(fused_features)

        return logits

# ===== ALTERNATIVE: CLIP-STYLE CONTRASTIVE LEARNING =====

class CLIPStyleMultiModal(nn.Module):
    """
    CLIP-style architecture for learning aligned text-image embeddings

    Training objective: Contrastive learning
    - Pull together embeddings of matching text-image pairs
    - Push apart embeddings of non-matching pairs

    Use case: Zero-shot classification, retrieval, image-text matching
    """
    def __init__(
        self,
        text_encoder_name: str = 'bert-base-uncased',
        image_encoder_name: str = 'resnet50',
        embedding_dim: int = 512,
        temperature: float = 0.07
    ):
        super().__init__()

        # Text encoder
        self.text_encoder = BertModel.from_pretrained(text_encoder_name)
        text_hidden_size = self.text_encoder.config.hidden_size

        # Image encoder
        resnet = models.resnet50(pretrained=True)
        self.image_encoder = nn.Sequential(*list(resnet.children())[:-1])
        image_hidden_size = 2048

        # Project to common embedding space
        self.text_projection = nn.Linear(text_hidden_size, embedding_dim)
        self.image_projection = nn.Linear(image_hidden_size, embedding_dim)

        # Temperature for contrastive loss
        self.temperature = temperature

    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        images: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Returns normalized embeddings for contrastive loss
        """
        # Text embedding
        text_output = self.text_encoder(input_ids, attention_mask).pooler_output
        text_embedding = self.text_projection(text_output)
        text_embedding = nn.functional.normalize(text_embedding, dim=-1)

        # Image embedding
        image_output = self.image_encoder(images).squeeze(-1).squeeze(-1)
        image_embedding = self.image_projection(image_output)
        image_embedding = nn.functional.normalize(image_embedding, dim=-1)

        return text_embedding, image_embedding

    def contrastive_loss(
        self,
        text_embeddings: torch.Tensor,
        image_embeddings: torch.Tensor
    ) -> torch.Tensor:
        """
        CLIP contrastive loss

        Args:
            text_embeddings: [batch_size, embedding_dim]
            image_embeddings: [batch_size, embedding_dim]

        Returns:
            loss: scalar
        """
        # Compute similarity matrix
        logits = torch.matmul(text_embeddings, image_embeddings.T) / self.temperature
        # [batch_size, batch_size]

        # Labels: diagonal is positive pairs
        batch_size = text_embeddings.size(0)
        labels = torch.arange(batch_size, device=text_embeddings.device)

        # Symmetric loss (textâ†’image and imageâ†’text)
        loss_text_to_image = nn.functional.cross_entropy(logits, labels)
        loss_image_to_text = nn.functional.cross_entropy(logits.T, labels)

        return (loss_text_to_image + loss_image_to_text) / 2

# ===== USAGE =====
if __name__ == "__main__":
    # Configuration
    BATCH_SIZE = 16
    NUM_CLASSES = 10
    DEVICE = 'cuda'

    # Initialize model
    model = MultiModalFusionModel(
        text_encoder_name='bert-base-uncased',
        image_encoder_name='resnet50',
        fusion_type='cross_attention',
        num_classes=NUM_CLASSES
    ).to(DEVICE)

    # Sample inputs
    from transformers import BertTokenizer
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    texts = ["a photo of a cat"] * BATCH_SIZE
    encoding = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')
    input_ids = encoding['input_ids'].to(DEVICE)
    attention_mask = encoding['attention_mask'].to(DEVICE)
    images = torch.randn(BATCH_SIZE, 3, 224, 224).to(DEVICE)

    # Forward pass
    logits = model(input_ids, attention_mask, images)
    print(f"Output shape: {logits.shape}")  # [16, 10]
```

**Key Design Choices:**

1. **Pre-trained Encoders**: Transfer learning from BERT (text) and ResNet/ViT (images)
2. **Projection Layers**: Map to common dimension for fusion
3. **Fusion Strategy**:
   - **Cross-attention**: Best for complex interactions (but slower)
   - **Concatenation**: Simple and fast
   - **Gated**: Learns importance of each modality
4. **Task Head**: Flexible for classification, generation, etc.

**When to Use Each Fusion Type:**
- **Cross-attention**: Tasks requiring deep modality interaction (VQA, image captioning)
- **Concatenation**: Simple tasks, limited compute
- **Gated/CLIP**: When modalities have variable quality/relevance

**Common Mistakes:**
- Not normalizing embeddings before fusion
- Different learning rates for pre-trained vs new layers
- Not handling variable-length sequences
- Forgetting to freeze early layers (catastrophic forgetting)

**Excellence Indicators:**
- Discusses recent architectures (CLIP, BLIP, Flamingo)
- Mentions modality-specific data augmentation
- Knows when to use contrastive vs supervised learning
- Considers computational trade-offs

---

#### Question 9: Advanced - Model Optimization and Deployment
**Question:** "You've trained a large language model that's too slow for production. Walk me through optimizing it for 10ms latency on CPU. What techniques would you use?"

**Comprehensive Answer:**

This question tests deep understanding of model optimization, deployment, and production constraints.

```python
import torch
import torch.nn as nn
from transformers import AutoModel, AutoTokenizer
import onnx
import onnxruntime as ort
from typing import List, Tuple
import numpy as np

class ModelOptimizationPipeline:
    """
    Complete pipeline for optimizing LLMs for production

    Techniques:
    1. Quantization (INT8, INT4)
    2. Pruning (structured/unstructured)
    3. Knowledge distillation
    4. ONNX export + optimization
    5. Dynamic batching
    6. KV cache optimization
    """

    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = AutoModel.from_pretrained(model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

    # ===== TECHNIQUE 1: QUANTIZATION =====
    def quantize_model(self, quantization_type: str = 'dynamic') -> nn.Module:
        """
        Quantization: Reduce precision from FP32 to INT8

        Benefits:
        - 4x smaller model size
        - 2-4x faster inference on CPU
        - Minimal accuracy loss (<1% typically)

        Types:
        - Dynamic: Quantize weights, activations computed at runtime
        - Static: Quantize both weights and activations (needs calibration data)
        - QAT: Quantization-aware training (best accuracy)
        """
        if quantization_type == 'dynamic':
            # Dynamic quantization (easiest, good for LLMs)
            quantized_model = torch.quantization.quantize_dynamic(
                self.model,
                {nn.Linear},  # Quantize all Linear layers
                dtype=torch.qint8
            )
            print("Dynamic quantization complete")
            return quantized_model

        elif quantization_type == 'static':
            # Static quantization (best performance, needs calibration)
            self.model.eval()
            self.model.qconfig = torch.quantization.get_default_qconfig('fbgemm')

            # Prepare for quantization
            model_prepared = torch.quantization.prepare(self.model)

            # Calibration step (run sample data through model)
            # This measures activation distributions
            sample_inputs = self._get_calibration_data()
            with torch.no_grad():
                for inputs in sample_inputs:
                    model_prepared(**inputs)

            # Convert to quantized model
            quantized_model = torch.quantization.convert(model_prepared)
            print("Static quantization complete")
            return quantized_model

    def _get_calibration_data(self) -> List[dict]:
        """Get representative data for calibration"""
        texts = ["Sample text " + str(i) for i in range(100)]
        inputs = [self.tokenizer(t, return_tensors='pt') for t in texts]
        return inputs

    # ===== TECHNIQUE 2: PRUNING =====
    def prune_model(self, sparsity: float = 0.3) -> nn.Module:
        """
        Pruning: Remove least important weights

        Benefits:
        - Reduces model size
        - Faster inference (with sparse kernels)
        - Can maintain accuracy with structured pruning

        Types:
        - Unstructured: Remove individual weights (flexible but hardware support limited)
        - Structured: Remove entire channels/heads (better hardware support)
        """
        import torch.nn.utils.prune as prune

        # Unstructured pruning (L1 norm)
        for name, module in self.model.named_modules():
            if isinstance(module, nn.Linear):
                prune.l1_unstructured(module, name='weight', amount=sparsity)
                # Make pruning permanent
                prune.remove(module, 'weight')

        print(f"Pruned {sparsity*100}% of weights")
        return self.model

    # ===== TECHNIQUE 3: KNOWLEDGE DISTILLATION =====
    def distill_model(
        self,
        student_model: nn.Module,
        train_dataloader,
        num_epochs: int = 3,
        temperature: float = 2.0,
        alpha: float = 0.5
    ) -> nn.Module:
        """
        Knowledge distillation: Train smaller model to mimic larger model

        Benefits:
        - 2-10x smaller model
        - Similar accuracy to large model
        - Much faster inference

        Example: DistilBERT is 40% smaller and 60% faster than BERT
        """
        teacher_model = self.model
        teacher_model.eval()
        student_model.train()

        optimizer = torch.optim.Adam(student_model.parameters(), lr=2e-5)
        ce_loss = nn.CrossEntropyLoss()
        kl_loss = nn.KLDivLoss(reduction='batchmean')

        for epoch in range(num_epochs):
            for batch in train_dataloader:
                # Get teacher predictions (soft labels)
                with torch.no_grad():
                    teacher_logits = teacher_model(**batch)
                    teacher_probs = nn.functional.softmax(teacher_logits / temperature, dim=-1)

                # Get student predictions
                student_logits = student_model(**batch)
                student_log_probs = nn.functional.log_softmax(student_logits / temperature, dim=-1)

                # Distillation loss (KL divergence between teacher and student)
                distill_loss = kl_loss(student_log_probs, teacher_probs) * (temperature ** 2)

                # Hard label loss (if available)
                if 'labels' in batch:
                    hard_loss = ce_loss(student_logits, batch['labels'])
                else:
                    hard_loss = 0

                # Combined loss
                loss = alpha * distill_loss + (1 - alpha) * hard_loss

                # Backprop
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

        return student_model

    # ===== TECHNIQUE 4: ONNX EXPORT + OPTIMIZATION =====
    def export_to_onnx(
        self,
        output_path: str = 'model.onnx',
        optimize: bool = True
    ) -> str:
        """
        Export to ONNX for faster inference

        Benefits:
        - Hardware-agnostic format
        - Optimized runtime (ONNX Runtime)
        - 2-3x faster than PyTorch on CPU
        - Graph-level optimizations
        """
        self.model.eval()

        # Sample input for tracing
        sample_text = "Sample input for export"
        inputs = self.tokenizer(sample_text, return_tensors='pt')

        # Export to ONNX
        torch.onnx.export(
            self.model,
            (inputs['input_ids'], inputs['attention_mask']),
            output_path,
            export_params=True,
            opset_version=14,
            input_names=['input_ids', 'attention_mask'],
            output_names=['output'],
            dynamic_axes={
                'input_ids': {0: 'batch_size', 1: 'sequence'},
                'attention_mask': {0: 'batch_size', 1: 'sequence'},
                'output': {0: 'batch_size'}
            }
        )

        if optimize:
            # ONNX graph optimizations
            from onnxruntime.transformers import optimizer
            optimized_model = optimizer.optimize_model(
                output_path,
                model_type='bert',
                num_heads=12,
                hidden_size=768
            )
            optimized_model.save_model_to_file(output_path)

        print(f"Model exported to {output_path}")
        return output_path

    # ===== TECHNIQUE 5: ONNX RUNTIME INFERENCE =====
    def create_onnx_session(self, onnx_path: str) -> ort.InferenceSession:
        """
        Create optimized ONNX Runtime session

        Performance optimizations:
        - Inter/intra op parallelism
        - Memory pattern optimization
        - Graph-level fusion
        """
        sess_options = ort.SessionOptions()

        # Performance optimizations
        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        sess_options.intra_op_num_threads = 4  # Adjust based on CPU cores
        sess_options.inter_op_num_threads = 4
        sess_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL

        # Create session
        session = ort.InferenceSession(
            onnx_path,
            sess_options=sess_options,
            providers=['CPUExecutionProvider']  # or TensorrtExecutionProvider for GPU
        )

        return session

    def onnx_inference(
        self,
        session: ort.InferenceSession,
        text: str
    ) -> np.ndarray:
        """Fast inference with ONNX Runtime"""
        inputs = self.tokenizer(text, return_tensors='np')

        ort_inputs = {
            'input_ids': inputs['input_ids'],
            'attention_mask': inputs['attention_mask']
        }

        # Run inference
        outputs = session.run(None, ort_inputs)
        return outputs[0]

# ===== TECHNIQUE 6: LAYER FUSION & GRAPH OPTIMIZATION =====

class FusedBERTLayer(nn.Module):
    """
    Fused BERT layer for faster inference

    Optimizations:
    - Fuse QKV projection into single matrix multiplication
    - Fuse LayerNorm + Dropout
    - Pre-compute static values
    """
    def __init__(self, original_layer):
        super().__init__()

        # Fuse Q, K, V projections
        self.qkv_proj = nn.Linear(
            original_layer.attention.self.query.in_features,
            original_layer.attention.self.query.out_features * 3
        )
        # Copy weights
        self.qkv_proj.weight.data = torch.cat([
            original_layer.attention.self.query.weight,
            original_layer.attention.self.key.weight,
            original_layer.attention.self.value.weight
        ], dim=0)

        # Rest of the layer...

    def forward(self, hidden_states, attention_mask):
        # Single matrix multiplication for Q, K, V
        qkv = self.qkv_proj(hidden_states)
        q, k, v = torch.chunk(qkv, 3, dim=-1)
        # ... rest of attention computation

# ===== TECHNIQUE 7: KV CACHE FOR AUTOREGRESSIVE GENERATION =====

class CachedInference:
    """
    KV cache for autoregressive generation

    Avoids recomputing Key and Value for previous tokens
    Speedup: 5-10x for long sequences
    """
    def __init__(self, model):
        self.model = model
        self.kv_cache = None

    def generate_with_cache(
        self,
        input_ids: torch.Tensor,
        max_length: int = 50
    ) -> torch.Tensor:
        """Generate with KV caching"""
        past_key_values = None

        for _ in range(max_length):
            # Only process new tokens
            if past_key_values is not None:
                inputs = input_ids[:, -1:]  # Only last token
            else:
                inputs = input_ids

            # Forward pass with cache
            outputs = self.model(
                inputs,
                past_key_values=past_key_values,
                use_cache=True
            )

            # Update cache
            past_key_values = outputs.past_key_values

            # Get next token
            next_token = outputs.logits[:, -1, :].argmax(dim=-1, keepdim=True)
            input_ids = torch.cat([input_ids, next_token], dim=1)

            # Check for EOS
            if next_token.item() == self.model.config.eos_token_id:
                break

        return input_ids

# ===== COMPLETE OPTIMIZATION WORKFLOW =====

def optimize_for_production(
    model_name: str,
    target_latency_ms: float = 10.0
) -> Tuple[ort.InferenceSession, float]:
    """
    Complete optimization pipeline for <10ms latency

    Steps:
    1. Knowledge distillation (if model too large)
    2. Quantization (INT8)
    3. Pruning (optional)
    4. ONNX export with optimizations
    5. ONNX Runtime with tuned settings
    6. Benchmark and iterate
    """
    import time

    pipeline = ModelOptimizationPipeline(model_name)

    # Step 1: Distillation (if needed)
    # For BERT-base -> DistilBERT equivalent
    # student_model = create_smaller_architecture()
    # distilled_model = pipeline.distill_model(student_model, train_data)

    # Step 2: Quantization
    print("Quantizing model...")
    quantized_model = pipeline.quantize_model(quantization_type='dynamic')

    # Step 3: Export to ONNX
    print("Exporting to ONNX...")
    onnx_path = pipeline.export_to_onnx(optimize=True)

    # Step 4: Create optimized ONNX session
    print("Creating ONNX Runtime session...")
    session = pipeline.create_onnx_session(onnx_path)

    # Step 5: Benchmark
    print("Benchmarking...")
    test_text = "This is a test sentence for benchmarking."
    latencies = []

    for _ in range(100):
        start = time.time()
        _ = pipeline.onnx_inference(session, test_text)
        latency = (time.time() - start) * 1000  # ms
        latencies.append(latency)

    p50_latency = np.percentile(latencies, 50)
    p95_latency = np.percentile(latencies, 95)
    p99_latency = np.percentile(latencies, 99)

    print(f"Latency - P50: {p50_latency:.2f}ms, P95: {p95_latency:.2f}ms, P99: {p99_latency:.2f}ms")

    if p95_latency > target_latency_ms:
        print(f"Warning: P95 latency ({p95_latency:.2f}ms) exceeds target ({target_latency_ms}ms)")
        print("Consider:")
        print("  - Further quantization (INT4)")
        print("  - Smaller model architecture")
        print("  - Hardware acceleration (GPU/TPU)")
        print("  - Caching frequent queries")

    return session, p95_latency

# Usage
if __name__ == "__main__":
    session, latency = optimize_for_production(
        'bert-base-uncased',
        target_latency_ms=10.0
    )
    print(f"Optimized model achieves {latency:.2f}ms latency")
```

**Complete Optimization Strategy for <10ms:**

1. **Architecture Choice**:
   - Start with DistilBERT (40% smaller than BERT)
   - Or MobileBERT for extreme efficiency
   - Consider TinyBERT for <5ms targets

2. **Quantization** (2-4x speedup):
   - Dynamic INT8: Easiest, 2-3x faster
   - Static INT8: Best accuracy, 3-4x faster
   - INT4: Extreme cases, 4-5x faster (quality loss)

3. **ONNX + Optimizations** (2-3x speedup):
   - Graph fusion (combine ops)
   - Constant folding
   - Optimized kernels (oneDNN for CPU)

4. **Pruning** (1.5-2x speedup):
   - Structured pruning (remove attention heads)
   - Magnitude-based pruning

5. **Caching**:
   - Redis for embeddings of common queries
   - 90% cache hit rate â†’ 10x effective speedup

6. **Hardware**:
   - If CPU not enough: NVIDIA T4 GPU (~1ms)
   - Or AWS Inferentia chips

**Expected Speedup:**
- Original BERT-base on CPU: ~100ms
- After distillation: ~40ms (DistilBERT)
- After quantization: ~15ms
- After ONNX optimization: ~7ms
- **Final: ~7ms âœ“**

**Common Mistakes:**
- Quantizing without calibration data (poor quality)
- Not benchmarking on production hardware
- Optimizing before profiling (wrong bottleneck)
- Ignoring cache warming (cold start latency)

**Excellence Indicators:**
- Provides specific speedup numbers
- Discusses accuracy vs speed trade-offs
- Mentions hardware-specific optimizations
- Knows production deployment tools (TorchServe, TensorFlow Serving, ONNX Runtime)

---

### Summary: AI/ML Frameworks

**Critical Takeaways:**
- PyTorch dominates LLM/research, TensorFlow still common in production
- Custom training loops: gradient accumulation, mixed precision, scheduling
- Multi-modal architectures: encoders + fusion + task heads
- Production optimization: quantization, pruning, distillation, ONNX

**Study Priority:**
- **High**: PyTorch basics, custom training, optimization
- **Medium**: Multi-modal models, TensorFlow (if targeting Google/old systems)
- **Lower**: Advanced architectures (unless research role)

**Preparation Time:**
- PyTorch fundamentals: 2 weeks
- Advanced training techniques: 2 weeks
- Model optimization: 1-2 weeks
- Multi-modal: 1 week

---

Due to length constraints, I'll continue this comprehensive guide in a separate document. Let me create the complete guide now covering all sections.

